{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed09a63-d5bb-4552-97ec-9ddeab71de6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SMdM analysis of data, produced by Concatenate_and_find_peaks.ipynb script\n",
    "\n",
    "This program takes an .hdf5 file with xy coordinates of fluorescent peaks as an input and preform these steps:\n",
    "\n",
    "- Cluster point using Voronoi clusterng procedure\n",
    "- Align defined clusters along x-axis\n",
    "- Calculate displacements of fluorescent peaks between odd and even frames in accordance with SMdM\n",
    "- Fit displacement data in each cluster with normalized probability density function (more details in the code)\n",
    "- Make displacement maps for each cluster with desired bin size \n",
    "- Analyze selected regions in each cluster and perform displacement fitting in them\n",
    "\n",
    "Rijksuniversiteit Groningen, 2021\n",
    "\n",
    "C.M. Punter (c.m.punter@rug.nl)\n",
    "\n",
    "D.S. Linnik (d.s.linnik@rug.nl)\n",
    "\n",
    "W.M. Smigiel (w.m.smigiel@gmail.com)\n",
    "\n",
    "L. Mantovanelli (l.mantovanelli@rug.nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f250b5e-ef09-4e16-9c99-5bc402e81aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "##################### SETTING IMPORTANT PARAMETERS OF THE ANALYSIS #####################\n",
    "\n",
    "pixel_size = 100e-3 # in micrometers\n",
    "delta_t = 1.5 # in milliseconds\n",
    "delta_t /= 1000\n",
    "\n",
    "### Selecting the hdf5 file ###\n",
    "root_path = os.getcwd() # Determine path to the .hdf5 files\n",
    "\n",
    "# Find a hdf5 file in directory or initialize in mannualy \n",
    "path_hdf5 = \"No_file\"\n",
    "\n",
    "for out in os.listdir(root_path):\n",
    "    if out.endswith('.hdf5'):\n",
    "        path_hdf5 = r'{}'.format(out)       \n",
    "# path_hdf5 = r\"20201023_3.hdf5\" # Mannualy determine .hdf5 file\n",
    "if path_hdf5 == \"No_file\":\n",
    "    raise Exception('No hdf5 file is found in current directory')\n",
    "\n",
    "### Parameters that are used for the voronoi clustering of cells ###\n",
    "\n",
    "cluster_density_factor = 0.15\n",
    "cluster_min_size = 100\n",
    "\n",
    "### Parameters that are used for pairing peaks ###\n",
    "\n",
    "r_max = 0.6 # displasements restrictions in micrometers\n",
    "background_correction = True\n",
    "\n",
    "filter_pairs= not background_correction # when background correction is not used, ambiguous pairs are filtered\n",
    "\n",
    "# initial parameters that used for fitting of the data\n",
    "initial_D = 1            # initial diffusion coefficients used for fitting      \n",
    "initial_b = 0            # initial background correction for fitting\n",
    "    \n",
    "# If all the clustered point clouds should be analysed\n",
    "analyse_all_cells = True\n",
    "\n",
    "# parameters for making diffusion maps\n",
    "pixel_bin_size_nm = 200      # in nm\n",
    "pixel_bin_size = pixel_bin_size_nm*100e-5 # in micrometers\n",
    "outlier_correction = True # For Tukey's fences outlier detection \n",
    "k_value = 3  # k value for Tukey's fences outlier detection  \n",
    "\n",
    "# parameters for poles selection\n",
    "Area_selection = True    # do poles need to be analysed\n",
    "persantage_of_pole = 0.2 # persantage of a pole in a cell - default value is 0.2\n",
    "\n",
    "\n",
    "################## END OF SETTING IMPORTANT PARAMETERS OF THE ANALYSIS #################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "# Create a tree of directorys for further storage of outputs\n",
    "\n",
    "# Remove already made folders inside \"Plots\" folder (Exsept plots for points densety mappng,clustering and aligned cells)\n",
    "# Some parts can be commented in order not to delete info while reanalyzing data \n",
    "try:\n",
    "    shutil.rmtree(root_path+\"/\"+path_hdf5[:-5]+'/Plots/Global fitting of cells')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(root_path+\"/\"+path_hdf5[:-5]+'/Plots/Fitting of areas')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(root_path+\"/\"+path_hdf5[:-5]+'/Plots/Diffusion maps')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Remove made folder for CSV files\n",
    "# Can be commented in order not to delete info while reanalyzing data \n",
    "try:\n",
    "    shutil.rmtree(root_path+\"/\"+path_hdf5[:-5]+'/CSV files')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Creating new folders to save analysis output\n",
    "os.chdir(root_path)\n",
    "try:\n",
    "    os.mkdir(path_hdf5[:-5])\n",
    "except FileExistsError:\n",
    "    pass\n",
    "os.chdir(path_hdf5[:-5])\n",
    "try:\n",
    "    os.mkdir(\"Intermediate files\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(\"CSV files\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(\"Plots\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "os.chdir(\"Plots\")\n",
    "try:\n",
    "    os.mkdir(\"Global fitting of cells\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(\"Diffusion maps\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00da75-6d6e-429f-a587-de6120ecbc07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Open the HDF5 file with all the fitted peaks and determine the total number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4048c18-2db6-4350-b209-298296dbb796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "f = h5py.File(path_hdf5, 'r')\n",
    "\n",
    "# get all frame numbers from the hdf5 file\n",
    "frames = [int(key[3:]) for key in f.keys() if key.startswith(\"fr_\")]\n",
    "n_frames = max(frames)\n",
    "print(\"Total number of frames is %a\" % (n_frames))\n",
    "\n",
    "\n",
    "# get all x, y coordinates from the HDF5 file\n",
    "x = []\n",
    "y = []\n",
    "frame = []\n",
    "\n",
    "for i in range(0, n_frames): \n",
    "    if \"fr_%d\" % i in f:\n",
    "        x1 = np.array(f[\"fr_%d\" % i]['x'])\n",
    "        y1 = np.array(f[\"fr_%d\" % i]['y'])\n",
    "    else:\n",
    "        x1 = np.array(())\n",
    "        y1 = np.array(())\n",
    "    x.extend(x1)\n",
    "    y.extend(y1)\n",
    "    frame.extend([i] * x1.size)\n",
    "\n",
    "frame = np.array(frame)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "print(\"Number of frames in analysis is %a\" % (n_frames))\n",
    "\n",
    "# plot 2D histogram to visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(10)\n",
    "h = ax.hist2d(x, y, (round(x.max() - x.min()), round(y.max() - y.min())), cmap=\"Greys\")\n",
    "# plt.colorbar(h[3], ax=ax) # optional adding of a colorbar on the left\n",
    "ax.set(xlabel='x', ylabel='y', title='Localizations')\n",
    "\n",
    "# savinf of a plot to folder \"Plots\"\n",
    "name_of_dir_to_save = path_hdf5[:-5]+\"/Plots/\"\n",
    "plt.savefig(name_of_dir_to_save+path_hdf5[:-5]+', Localization of cells.pdf')\n",
    "\n",
    "plt.show() # show plot in a notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b2e0a-d7f2-4602-851e-da9418c8f468",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We use voronoi clustering to detect cells. The voronoi clustering code is based on \n",
    "## https://github.com/ZhuangLab/storm-analysis/blob/master/storm_analysis/voronoi/voronoi_analysis.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44cecb5-109d-466e-94ab-50aff152f4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# based on https://github.com/ZhuangLab/storm-analysis/blob/master/storm_analysis/voronoi/voronoi_analysis.py\n",
    "def voronoi_clustering(x, y, density_factor, min_size):\n",
    "\n",
    "    points = np.stack((x, y), axis=-1)\n",
    "    vor = Voronoi(points) \n",
    "    \n",
    "    # https://en.wikipedia.org/wiki/Shoelace_formula\n",
    "    def polygon_area(x, y):\n",
    "        return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
    "\n",
    "    # determine density of each region\n",
    "    density = np.zeros(x.size)\n",
    "    labels = np.zeros(x.size, dtype = np.int32) - 1\n",
    "    \n",
    "    for i, region_index in enumerate(vor.point_region):\n",
    "        vertices = []\n",
    "\n",
    "        for vertex in vor.regions[region_index]:\n",
    "            if (vertex != -1):\n",
    "                vertices.append(vor.vertices[vertex])\n",
    "\n",
    "        vertices = np.array(vertices)\n",
    "        area = polygon_area(vertices[:,0], vertices[:,1])\n",
    "        density[i] = 1 / area\n",
    "\n",
    "    # determine minimum density threshold\n",
    "    median_density = np.median(density)\n",
    "    threshold_density = median_density * density_factor\n",
    "    \n",
    "    max_neighbors = 40\n",
    "    neighbors = np.zeros((x.size, max_neighbors), dtype = np.int32) - 1\n",
    "    neighbors_counts = np.zeros((x.size), dtype = np.int32)\n",
    "\n",
    "    for ridge_p in vor.ridge_points:\n",
    "        p1 = ridge_p[0]\n",
    "        p2 = ridge_p[1]\n",
    "\n",
    "        # add p2 to the list for p1\n",
    "        neighbors[p1,neighbors_counts[p1]] = p2\n",
    "        neighbors_counts[p1] += 1\n",
    "\n",
    "        # add p1 to the list for p2\n",
    "        neighbors[p2,neighbors_counts[p2]] = p1\n",
    "        neighbors_counts[p2] += 1\n",
    "\n",
    "    min_density = density_factor * median_density\n",
    "    visited = np.zeros(x.size, dtype = np.int32)\n",
    "\n",
    "    def neighborsList(index):\n",
    "        nlist = []\n",
    "        for i in range(neighbors_counts[index]):\n",
    "            loc_index = neighbors[index,i]\n",
    "            if (visited[loc_index] == 0):\n",
    "                nlist.append(neighbors[index,i])\n",
    "                visited[loc_index] = 1\n",
    "        return nlist\n",
    "\n",
    "    cluster_id = 0\n",
    "    for i in range(x.size):\n",
    "        if (visited[i] == 0):\n",
    "            visited[i] = 1\n",
    "            if (density[i] > min_density):\n",
    "                cluster_elt = [i]\n",
    "                c_size = 1\n",
    "                to_check = neighborsList(i)\n",
    "                while (len(to_check) > 0):\n",
    "\n",
    "                    # remove last localization from the list.\n",
    "                    loc_index = to_check[-1]\n",
    "                    to_check = to_check[:-1]\n",
    "\n",
    "                    # if the localization has sufficient density add to cluster and\n",
    "                    # check neighbors.\n",
    "                    if (density[loc_index] > min_density):\n",
    "                        to_check += neighborsList(loc_index)\n",
    "                        cluster_elt.append(loc_index)\n",
    "                        c_size += 1\n",
    "\n",
    "                    # mark as visited.\n",
    "                    visited[loc_index] = 1\n",
    "\n",
    "                # mark the cluster if there are enough localizations in the cluster.\n",
    "                if (c_size > min_size):\n",
    "                    for elt in cluster_elt:\n",
    "                        labels[elt] = cluster_id\n",
    "                    cluster_id += 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "cell = voronoi_clustering(x, y, cluster_density_factor, cluster_min_size)\n",
    "\n",
    "# plot all found clusters\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(10)\n",
    "ax.hist2d(x, y, (round(x.max() - x.min()), round(y.max() - y.min())), cmap='Greys')\n",
    "ax.scatter(x[cell >= 0], y[cell >= 0], c=cell[cell >= 0], s=5, cmap=\"Paired\")\n",
    "ax.set(xlabel='x', ylabel='y', title='Clustered Cells')\n",
    "\n",
    "# show cluster number on a plot\n",
    "for c in np.unique(cell):\n",
    "    if c >= 0:\n",
    "        mask = cell == c\n",
    "        x1 = x[mask].mean()\n",
    "        y1 = y[mask].mean()\n",
    "        ax.text(x1, y1, f\"$\\\\bf{c}$\", color=\"red\", size = 15)\n",
    "\n",
    "# save plot in folder \"Plots\"\n",
    "name_of_dir_to_save = path_hdf5[:-5]+\"/Plots/\"\n",
    "plt.savefig(name_of_dir_to_save+path_hdf5[:-5]+', Voronoi clustering of cells.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# filter peaks that are not within a cluster\n",
    "frame = frame[cell >= 0]\n",
    "x = x[cell >= 0]\n",
    "y = y[cell >= 0]\n",
    "cell = cell[cell >= 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86626533-a4d6-4bdf-bc44-0d7bb742e518",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Align all cells along the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2aef4f-7bc2-4c3a-b860-4a01dc557b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rotate(theta, points):\n",
    "    \"\"\"\n",
    "        Rotates the array of (x,y) points at angle theta (in radians) using rotation matrix\n",
    "    \"\"\" \n",
    "    r = np.array(((np.cos(theta), -np.sin(theta)),\n",
    "                 (np.sin(theta),  np.cos(theta))))\n",
    "    return points.dot(r)\n",
    "\n",
    "# https://alyssaq.github.io/2015/computing-the-axes-or-orientation-of-a-blob/\n",
    "def find_orientation(points):\n",
    "    \"\"\"\n",
    "        Gets the angle of first principle component of a 2D pointcloud as an \n",
    "        angle of the first eigenvector of a covariance matrix of xy points  \n",
    "    \"\"\"\n",
    "    cov = np.cov(points[:,0], points[:,1])\n",
    "    evals, evecs = np.linalg.eig(cov)\n",
    "    sort_indices = np.argsort(evals)\n",
    "    x, y = evecs[:, sort_indices[1]]    \n",
    "    return np.arctan2(y, x)\n",
    "\n",
    "\n",
    "# create a grid to plot each rotated cluster seperately\n",
    "number_of_cells = np.unique(cell).size\n",
    "cols = 4\n",
    "rows = ((number_of_cells - 1) // cols) + 1\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, sharex=True, sharey=True, figsize = (9, 9))\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "# initialize dictionaries to save intermediate data \n",
    "Dict_for_number_of_points = {}\n",
    "Dict_for_rotation_angles ={}\n",
    "\n",
    "# plotting rotated clusters\n",
    "for i, c in enumerate(np.unique(cell)):\n",
    "    cell_mask = cell == c\n",
    "    points = np.stack((x[cell_mask], y[cell_mask]), axis=1)\n",
    "    points -= np.mean(points, axis=0)\n",
    "    points *= pixel_size\n",
    "    \n",
    "    theta = find_orientation(points)        \n",
    "    points = rotate(theta, points)\n",
    "    x[cell_mask], y[cell_mask] = points.T\n",
    "    \n",
    "    Dict_for_number_of_points[c] = len(x[cell_mask])\n",
    "    Dict_for_rotation_angles[c] = round(theta*180/np.pi,2)\n",
    "    \n",
    "    ax = axes[i // cols, i % cols]\n",
    "    ax.scatter(x[cell_mask], y[cell_mask],s=0.1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    if theta >=0:\n",
    "        ax.set_title('Cell %d, ' % c + '{0:.0f} points, '.format(len(x[cell_mask])) + str(abs(Dict_for_rotation_angles[c])) +  chr(176) +'\\u2193')\n",
    "    else:\n",
    "        ax.set_title('Cell %d, ' % c + '{0:.0f} points, '.format(len(x[cell_mask])) + str(abs(Dict_for_rotation_angles[c])) +  chr(176) +'\\u2191')\n",
    "        \n",
    "\n",
    "for row in range(rows):\n",
    "    axes[row, 0].set_ylabel(\"\\u03BCm\")\n",
    "for col in range(cols):\n",
    "    axes[rows - 1, col].set_xlabel(\"\\u03BCm\")\n",
    "\n",
    "fig.suptitle(\"Aligned cells (\" + \"\\u2193 for clockwise rotation, \\u2191 for counter clockwise rotation )\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot in folder \"Plots\"\n",
    "name_of_dir_to_save = path_hdf5[:-5]+\"/Plots/\"\n",
    "plt.savefig(name_of_dir_to_save+path_hdf5[:-5]+', Table of alignes cells.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29152f4b-15c2-47aa-ac56-bd6e4edc8915",
   "metadata": {},
   "source": [
    "## Calculating displacements either with point discarding or background correction (based on setted parematers)\n",
    "## and saving intermediate files for reanalyzing ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e561419-0d3c-43b6-a407-53f44a2d2cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# asking user wether all clusters should be abalysed unless analyse_all_cells is TRUE\n",
    "if analyse_all_cells:\n",
    "    list_of_cells = list(np.unique(cell))\n",
    "else: \n",
    "    str_list_of_cells = input(\"Enter perspective cells numbers with space or all\\n\")  \n",
    "    if str_list_of_cells == \"all\":\n",
    "        list_of_cells = list(np.unique(cell))\n",
    "    else:\n",
    "        list_of_cells = [int(x) for x in str_list_of_cells.split()]\n",
    "\n",
    "counter_cells = 0\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "displacements = []\n",
    "if filter_pairs:\n",
    "    print(\"Discarding pairs outside radius of %a nm\" % (r_max*1000))\n",
    "    \n",
    "for c in np.unique(cell):\n",
    "    if c in list_of_cells:\n",
    "        \n",
    "        # setting the progress bar\n",
    "        percantage_to_text = int(counter_cells*100/len(list_of_cells))\n",
    "        percantage_to_plot = int(counter_cells*100/len(list_of_cells)/2)\n",
    "        print(\"Cell %a, \" %c +\"\\u25ae\" * percantage_to_plot + \"\\u25af\" * (50-percantage_to_plot) + \" \" + str(percantage_to_text) + \"% \\r\", end=\"\")\n",
    "        counter_cells += 1\n",
    "        \n",
    "        for fr in range(0, n_frames,2):\n",
    "                      \n",
    "            first = (frame == fr) & (cell == c)\n",
    "            second = (frame == (fr + 1)) & (cell == c)\n",
    "\n",
    "            x1, y1, x2, y2 = x[first], y[first], x[second], y[second]\n",
    "\n",
    "            # calculate a distance matrix\n",
    "            x_from, x_to = np.meshgrid(x1, x2)\n",
    "            y_from, y_to = np.meshgrid(y1, y2)\n",
    "            r = np.sqrt(((x_to - x_from) ** 2) + ((y_to - y_from) ** 2))\n",
    "\n",
    "            # check which coordinates are within r_max distance\n",
    "            connected = r <= r_max\n",
    "            valid = connected\n",
    "\n",
    "            if filter_pairs:\n",
    "                # make sure each coordinate is linked to exactly one other coordinate\n",
    "                single_from = np.sum(connected, axis=0) == 1\n",
    "                single_to = np.sum(connected, axis=1) == 1\n",
    "                valid = connected & np.logical_and(*np.meshgrid(single_from, single_to))\n",
    "\n",
    "            pairs = np.array((np.full(valid.sum(), fr), np.full(valid.sum(), c), x_from[valid], y_from[valid], x_to[valid], y_to[valid], r[valid])).T\n",
    "            displacements.extend(pairs)\n",
    "            \n",
    "\n",
    "print(\"Cell %a, \" % list_of_cells[-1] +\"\\u25ae\" * 50 + \" 100 %\\n\" )\n",
    "displacements = np.array(displacements)\n",
    "\n",
    "\n",
    "# write nessesary intermediate information into 4 separate files\n",
    "name_of_dir_to_save = path_hdf5[:-5]+\"/Intermediate files/\"\n",
    "\n",
    "a_file_points = open(name_of_dir_to_save+\"itm_Dict_for_number_of_points.pkl\", \"wb\")\n",
    "pickle.dump(Dict_for_number_of_points, a_file_points)                                    # Save information of number of points in each cluster\n",
    "a_file_points.close()\n",
    "\n",
    "a_file_angles = open(name_of_dir_to_save+\"itm_Dict_for_rotation_angles.pkl\", \"wb\")\n",
    "pickle.dump(Dict_for_rotation_angles, a_file_angles)                                     # Save information of angle of rotation for each cluster\n",
    "a_file_angles.close()\n",
    "\n",
    "np.save(name_of_dir_to_save+\"itm_Displasements\", displacements)                          # Save information about displasements in each cluster\n",
    "\n",
    "np.save(name_of_dir_to_save+\"itm_List of Cells\", list_of_cells)                          # Save list of clusters for wgich we calculated displasements \n",
    "\n",
    "print(\"\\nSucsessfully saved intermediate files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdbeb7-1a04-42ce-ae97-46cecfe9a77a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using the following probability density function (pdf) based on Rayleigh distribution: \n",
    "\n",
    "$$ P(r,a)=\\frac{2r}{a}e^{-\\frac{r^2}{a}} $$\n",
    "\n",
    "where $a=4\\Delta t D$ and displacements $r$ we can use maximum-likelihood estimation to find diffusion coefficient $D$.\n",
    "\n",
    "Some displacements originate from noise. The probability density of such displacements should be added to the probability density function (pdf) as a linear summation $br$.\n",
    "\n",
    "$$  P(r,a)=\\frac{2r}{a}e^{-\\frac{r^2}{a}}+br $$\n",
    "\n",
    "When b is a positive, non zero, number the total total probability density will exceed 1.0. Also, when restricting r_max, the total density at r_max may be lower than 1.0. Since we consider displacements limited by $r_{max}$, $P(r,a,b)$ should be normalized by $\\int_{0}^{r_{max}} P(r,a,b)\\,dx$.\n",
    "\n",
    "$$ \\int_{0}^{r_{max}} \\frac{2r}{a}e^{-\\frac{r^2}{a}}+br \\,dx = \\int_{0}^{r_{max}} \\frac{2r}{a}e^{-\\frac{r^2}{a}} \\,dx +\\int_{0}^{r_{max}} br \\,dx = 1 - e^{-\\frac{r_{max}^2}{a}} + \\frac{r_{max}^2b}{2}$$\n",
    "\n",
    "\n",
    "Since the total probability density is 1 the normalized probability density fuction can be written as:\n",
    "\n",
    "$$ P(r,a,b)=\\frac{\\frac{2r}{a}e^{-\\frac{r^2}{a}}+br}{1 - e^{-\\frac{r_{max}^2}{a}}+ \\frac{r_{max}^2b}{2}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74824dc-e79b-4c67-b6f0-ee708119d19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "###              IF RUNNING STARTING ONLY FROM THIS PART OF ANALYSIS, PLEASE REMEMBER TO INITIALIZE PARAMETERS AT THE VERY BEGINING OF A SCRIPT\n",
    "###              IT WILL DELETE SOME OF THE INFORAMTION IN A ANALUSIS OUTPUT FILDER\n",
    "###\n",
    "\n",
    "# loading nessesary data drom previous analysis\n",
    "\n",
    "name_of_dir_to_load= path_hdf5[:-5]+\"/Intermediate files/\"\n",
    "\n",
    "with open(name_of_dir_to_load+'itm_Dict_for_number_of_points.pkl', 'rb') as handle_points:\n",
    "    Dict_for_number_of_points = pickle.load(handle_points)  \n",
    "\n",
    "with open(name_of_dir_to_load+'itm_Dict_for_rotation_angles.pkl', 'rb') as handle_angles:\n",
    "    Dict_for_rotation_angles = pickle.load(handle_angles)\n",
    "\n",
    "displacements = np.load(name_of_dir_to_load+'itm_Displasements.npy')\n",
    "list_of_cells = np.load(name_of_dir_to_load+'itm_List of Cells.npy')\n",
    "\n",
    "\n",
    "import scipy.optimize\n",
    "import scipy.integrate\n",
    "\n",
    "def pdf(r, r_max, delta_t, D, b):\n",
    "    \"\"\"\n",
    "    PDF for fitting issues\n",
    "    \"\"\"\n",
    "    a = 4 * delta_t * D\n",
    "    density = ((2 * r) / a) * np.exp(-(r ** 2) / a) + (b * r)\n",
    "    normalization = (1 - np.exp(-(r_max ** 2) / a)) + ((r_max ** 2) * b) / 2\n",
    "    return density / normalization\n",
    "\n",
    "def likelihood(r, r_max, delta_t, D, b):\n",
    "    \"\"\"\n",
    "    Getting summs of the Logs for MLE \n",
    "    \"\"\"\n",
    "    return np.sum(np.log(pdf(r, r_max, delta_t, D, b)))\n",
    "    \n",
    "def minimization_function(x, *args):\n",
    "    \"\"\"\n",
    "    Minimization of the MLE \n",
    "    Returns reversed value of likelihhod\n",
    "    \"\"\"\n",
    "    if background_correction:\n",
    "        D, b = x\n",
    "    else:\n",
    "        D, b = *x, 1\n",
    "        \n",
    "    r, r_max, delta_t = args\n",
    "    return -likelihood(r, r_max, delta_t, D, b)\n",
    "\n",
    "def maximum_likelihood(r, r_max, delta_t, D, b=0):\n",
    "    \"\"\"\n",
    "    Calculating parameters of the distribution (Diffusion coefficient and background correction coefficient)\n",
    "    Maximum of the likelihood function is evaluated by minimization of - likelihood function \"minimization_function(x, *args)\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if background_correction:\n",
    "        bounds = [(1e-6, None), (0, None)]\n",
    "        initial = (D, b)\n",
    "    else:\n",
    "        bounds =  [(1e-6, None)]\n",
    "        initial = (D,)\n",
    "        \n",
    "    res = scipy.optimize.minimize(minimization_function, initial, (r, r_max, delta_t), method='Nelder-Mead', bounds=bounds)\n",
    "    \n",
    "    return res.x\n",
    "\n",
    "  \n",
    "rows = len(list_of_cells)\n",
    "cols = 2\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize = (19, rows*7))\n",
    "\n",
    "\n",
    "# creating a dataframe to save output data as .csv file\n",
    "df = pd.DataFrame(columns=['Cell_id', 'Number_of_points','Angle_of_rotation', 'Direction_of_rotation', 'Number_of_displasements', \"D_value\", \"b_value\", \"length, μm\", \"height, μm\"])\n",
    "name_for_df = path_hdf5[:-5]+\", Cells info (sep = %.1f ms, pixel size  = %.0f nm).csv\" % (delta_t*1000, int(pixel_size*1000)) \n",
    "\n",
    "\n",
    "for i, c in enumerate(list_of_cells):                                                                  \n",
    "    \n",
    "    fig_to_save, axes_to_save = plt.subplots(nrows = 1, ncols = 2, figsize = (19, 7))\n",
    "    \n",
    "    cell_displacements = displacements[:,1] == c\n",
    "    _, _, x1, y1, x2, y2, r = displacements[cell_displacements].T\n",
    "    \n",
    "    D, b = maximum_likelihood(r, r_max, delta_t, initial_D, initial_b)\n",
    "    \n",
    "    x_fitted = np.linspace(0, r_max, 100)\n",
    "    y_fitted = pdf(x_fitted, r_max, delta_t, D, b)\n",
    "\n",
    "    if len(list_of_cells) == 1:\n",
    "        ax = axes[0]\n",
    "    else:\n",
    "        ax = axes[i, 0]\n",
    "    ax.hist(r, density=True, bins=\"doane\", label=\"Binned displacements\", alpha = 0.6)\n",
    "    axes_to_save[0].hist(r, density=True, bins=\"doane\", label=\"Binned displacements\", alpha = 0.6)\n",
    "    \n",
    "    if background_correction:\n",
    "        label_g = \"PDF with b=%.2f\" % b\n",
    "    else:\n",
    "        label_g = \"\"\n",
    "\n",
    "    ax.plot(x_fitted, y_fitted, label=label_g, c = \"red\", linewidth=2)\n",
    "    \n",
    "    y_fitted_one = pdf(np.linspace(0, r_max, 100), r_max, delta_t, D, b)\n",
    "    ax.plot(np.linspace(0, r_max, 100), y_fitted_one, c=\"red\", label=(r\"D=%.2f\" % (D,)), linewidth=2)\n",
    "    axes_to_save[0].plot(np.linspace(0, r_max, 100), y_fitted_one, c=\"red\", label=(r\"D=%.2f\" % (D,)), linewidth=2)\n",
    "            \n",
    "    Row_to_add_to_df = []\n",
    "    Row_to_add_to_df.append(int(c))\n",
    "    Row_to_add_to_df.append(Dict_for_number_of_points[c])\n",
    "    Row_to_add_to_df.append(abs(Dict_for_rotation_angles[c]))\n",
    "    if Dict_for_rotation_angles[c] >= 0:\n",
    "        Row_to_add_to_df.append('Clockwise')\n",
    "    else:\n",
    "        Row_to_add_to_df.append('Counterclockwise')\n",
    "    Row_to_add_to_df.append(len(x1))\n",
    "    Row_to_add_to_df.append(round(D,2))\n",
    "    Row_to_add_to_df.append(round(b,2))\n",
    "    Row_to_add_to_df.append(np.amax(x1)-np.amin(x1))\n",
    "    Row_to_add_to_df.append(np.amax(y1)-np.amin(y1))\n",
    "    Row_as_serias = pd.Series(Row_to_add_to_df, index = df.columns)\n",
    "    df = df.append(Row_as_serias, ignore_index=True)\n",
    "        \n",
    "    ax.set(xlabel='Displacement', ylabel='Probability', title=\"Cell %d\" % c)\n",
    "    axes_to_save[0].set(xlabel='Displacement', ylabel='Probability', title=\"Cell %d\" % c)\n",
    "    ax.grid()\n",
    "    axes_to_save[0].grid()\n",
    "    ax.legend(fontsize=15)\n",
    "    axes_to_save[0].legend(fontsize=15)\n",
    "\n",
    "    if len(list_of_cells) == 1:\n",
    "        ax = axes[1]\n",
    "    else:\n",
    "        ax = axes[i, 1]\n",
    "    ax.scatter(x1, y1)\n",
    "    axes_to_save[1].scatter(x1, y1, c=\"orange\")\n",
    "    ax.set(xlabel='\\u03BCm', ylabel='\\u03BCm', title='Localizations of '+ '{0:.0f} start positions of displacements'.format(len(x1)))\n",
    "    axes_to_save[1].set(xlabel='\\u03BCm', ylabel='\\u03BCm', title='Localizations of '+ '{0:.0f} start positions of displacements'.format(len(x1)))\n",
    "    ax.axis('equal')\n",
    "    axes_to_save[1].axis('equal')\n",
    "    ax.grid()\n",
    "    axes_to_save[1].grid()\n",
    "    name_of_dir_to_save_plots = path_hdf5[:-5]+\"/Plots/Global fitting of cells/\"\n",
    "    plt.savefig(name_of_dir_to_save_plots+path_hdf5[:-5]+\", Fitting of Cell %a.pdf\" % c)\n",
    "    plt.close(fig_to_save)\n",
    "\n",
    "dir_to_save_csv = path_hdf5[:-5]+\"/CSV files/\"\n",
    "    \n",
    "df.to_csv(dir_to_save_csv + name_for_df, index = False, header=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53bdec-e7e7-47df-89c9-e8bc0feeb289",
   "metadata": {},
   "source": [
    "## Make displacement map and analyze cell regions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df244af-8890-4b54-8345-07f757c284a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Binning2D:\n",
    "    \"\"\"\n",
    "        This class bins the data by taking all displacements that lie within a given rectangle (x0, y0, x1, y1).\n",
    "        The class also contains methods to get the minimum and maximum coordinates (get_bounds) and to get a grid of bins (get_bins).\n",
    "        \n",
    "        The get_bins method takes an area (x0, y0, x1, y1) and a pixel size (bin_width) and returns a matrix where each column consists of a list of displacements\n",
    "        (i.e. the upper left column contains a list of displacements that start on a coordinate that falls within (x0, y0, x0 + bin_width, y0 + bin_width)) .\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, data):\n",
    "        self.data = data\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.x_indices = np.argsort(self.x)\n",
    "        self.y_indices = np.argsort(self.y)\n",
    "        self.x_values_sorted = np.sort(self.x)\n",
    "        self.y_values_sorted = np.sort(self.y)\n",
    "    \n",
    "    def get_data(self, x0, y0, x1, y1):\n",
    "        x_left = np.searchsorted(self.x_values_sorted, x0, side=\"left\")\n",
    "        x_right = np.searchsorted(self.x_values_sorted, x1, side=\"right\")\n",
    "        y_left = np.searchsorted(self.y_values_sorted, y0, side=\"left\")\n",
    "        y_right = np.searchsorted(self.y_values_sorted, y1, side=\"right\")\n",
    "        \n",
    "        indices = np.intersect1d(self.x_indices[x_left:x_right], self.y_indices[y_left:y_right])\n",
    "        return self.x[indices], self.y[indices], self.data[indices]\n",
    "    \n",
    "    def get_bounds(self):\n",
    "        x_min, x_max = self.x_values_sorted[0], self.x_values_sorted[-1]\n",
    "        y_min, y_max = self.y_values_sorted[0], self.y_values_sorted[-1]\n",
    "        return x_min, y_min, x_max, y_max\n",
    "    \n",
    "    def get_bins(self, x0, y0, x1, y1, pixel_bin_size):\n",
    "        rows = []\n",
    "        for y in np.arange(y0, y1, pixel_bin_size):    \n",
    "            cols = []\n",
    "            for x in np.arange(x0, x1, pixel_bin_size):\n",
    "                cols.append(self.get_data(x, y, x + pixel_bin_size, y + pixel_bin_size))\n",
    "            rows.append(cols)\n",
    "        return rows\n",
    "\n",
    "    \n",
    "def Cell_slicing():\n",
    "    \"\"\"\n",
    "    Divides the analysid clustered pointcloud in three areas based on the \"persantage_of_pole\" based on the binning size of the points\n",
    "    Calculates diffusion coefficients and backgroung correction in each area, plots them and saves in named folders for each cell and area\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing the borderes of an analysed cell and borders of areas\n",
    "    \n",
    "    width_of_poles = int(round(width*persantage_of_pole,0))\n",
    "    width_center = int(width - 2 * width_of_poles)\n",
    "\n",
    "    area_boundarys = [[(0,start_offset),(height, start_offset+width_of_poles)],\n",
    "                      [(0,start_offset+width_of_poles+1),(height,start_offset+width_of_poles+width_center)],\n",
    "                      [(0,start_offset+width_of_poles+width_center+1),(height,start_offset+width_of_poles+width_center+width_of_poles)]]\n",
    "    \n",
    "    \n",
    "    x_area_borders = []\n",
    "    x_area_borders.append(area_boundarys[0][0][1])\n",
    "    for i in range(0, len(area_boundarys)):\n",
    "        x_area_borders.append(area_boundarys[i][1][1])\n",
    "    x_area_borders = [binning.get_bounds()[0] + i * pixel_bin_size for i in x_area_borders]\n",
    "    \n",
    "    y_area_borders = []\n",
    "    y_area_borders.append(area_boundarys[0][0][0])\n",
    "    y_area_borders.append(area_boundarys[0][1][0])\n",
    "    y_area_borders = [binning.get_bounds()[1] + i * pixel_bin_size for i in y_area_borders]\n",
    "\n",
    "\n",
    "    for area_number in range(0,3): \n",
    "        \n",
    "        try:\n",
    "            os.mkdir(\"Area %a\" % (area_number +1))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        dir_to_save = \"Area %a\" % (area_number+1)\n",
    "        \n",
    "        # initializing x,y and r in selected area\n",
    "        \n",
    "        x1_area = x1[x1 > x_area_borders[area_number]]\n",
    "        x1_area = x1_area[ x1_area < x_area_borders[area_number+1]]\n",
    "\n",
    "        r_area = np.where(x1>x_area_borders[area_number], r, np.nan)\n",
    "        r_area = np.where(x1<x_area_borders[area_number+1], r_area, np.nan)\n",
    "        r_area = r_area[np.logical_not(np.isnan(r_area))]\n",
    "\n",
    "        y_area = np.where(x1>x_area_borders[area_number], y1, np.nan)\n",
    "        y_area = np.where(x1<x_area_borders[area_number+1], y_area, np.nan)\n",
    "        y_area = y_area[np.logical_not(np.isnan(y_area))]\n",
    "    \n",
    "        # plotting the maps with area borders and scatter plot of analysed displasements\n",
    "        \n",
    "        plt.figure(figsize=(15,2))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(fitted_bins, origin=\"lower\", aspect='equal', extent=[binning.get_bounds()[0],\n",
    "                                                                    binning.get_bounds()[0] + pixel_bin_size * len(fitted_bins[0]),\n",
    "                                                                    binning.get_bounds()[1],\n",
    "                                                                    binning.get_bounds()[1] + pixel_bin_size * fitted_bins.shape[0]])\n",
    "        plt.vlines(x_area_borders, min(y_area_borders),max(y_area_borders), color = \"red\")\n",
    "        plt.title('Diffusion map')\n",
    "        plt.colorbar()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(counts, origin=\"lower\", aspect='equal', extent=[binning.get_bounds()[0],\n",
    "                                                                        binning.get_bounds()[0] + pixel_bin_size * len(fitted_bins[0]),\n",
    "                                                                        binning.get_bounds()[1],\n",
    "                                                                        binning.get_bounds()[1] + pixel_bin_size * fitted_bins.shape[0]])\n",
    "        plt.vlines(x_area_borders, min(y_area_borders),max(y_area_borders), color = \"red\")\n",
    "        plt.title('Number of displacements, area %a' % (area_number+1))\n",
    "        plt.colorbar()\n",
    "        plt.scatter(x1_area, y_area, s = 2, c=\"red\")\n",
    "        plt.savefig(dir_to_save+\"/\"+path_hdf5[:-5]+\", Diffusion map and displasement map + scatter of analysed points, cell %a, area %a\" % (c,area_number+1))                     \n",
    "        plt.show()\n",
    "        \n",
    "        # Fitting displacements in selected area\n",
    "        \n",
    "        D, b = maximum_likelihood(r_area, r_max, delta_t, initial_D, initial_b)\n",
    "        x_fitted = np.linspace(0, r_max, 100)\n",
    "        y_fitted = pdf(x_fitted, r_max, delta_t, D, b)\n",
    "        \n",
    "        # plotting the PDF MLE fitting\n",
    "        \n",
    "        fig, axes = plt.subplots(figsize=(9,7))\n",
    "        axes.hist(r_area, density=True, bins=\"doane\", alpha = 0.6)\n",
    "        axes.plot(np.linspace(0, r_max, 100), y_fitted, label=(r\"$D=%.2f\\: \\frac{\\mu m^{2}}{s}, b=%.2f$\" % (D, b)), linewidth=2)\n",
    "        axes.legend(fontsize=14)\n",
    "        axes.set(xlabel='Displacement', ylabel='Probability', title=\"Cell %a, Area %a\" %(c, area_number+1))\n",
    "        plt.savefig(dir_to_save+\"/\"+path_hdf5[:-5]+\", Fitting, cell %a, area %a\" % (c, area_number+1))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        Row_to_add_to_df_area.append(round(D,2))\n",
    "        Row_to_add_to_df_area.append(len(x1_area))\n",
    "        Row_to_add_to_df_area.append(round(b,2))\n",
    "\n",
    "        \n",
    "if Area_selection:        \n",
    "    df_area = pd.DataFrame(columns=['Cell_id', 'D_of_area_1', 'Number_of_points_in_area_1', 'b_of_area_1',\n",
    "                                               'D_of_area_2', 'Number_of_points_in_area_2', 'b_of_area_2', \n",
    "                                               'D_of_area_3', 'Number_of_points_in_area_3', 'b_of_area_2','Side'])\n",
    "    name_for_df_area = path_hdf5[:-5]+\", Areas analysis (bin pixel size = %a nm, poles are %a %%).csv\" % (pixel_bin_size_nm, int(persantage_of_pole*100))\n",
    "    \n",
    "    os.chdir(path_hdf5[:-5])\n",
    "    os.chdir('Plots')\n",
    "    try:\n",
    "        os.mkdir(\"Fitting of areas\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    os.chdir('Fitting of areas')\n",
    "    for i in list_of_cells:\n",
    "        try:\n",
    "            os.mkdir(\"Area selection in cell %a\" % i)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    os.chdir(root_path)\n",
    "        \n",
    "for i, c in enumerate(list_of_cells):\n",
    "    \n",
    "    cell_displacements = displacements[:,1] == c\n",
    "    _, _, x1, y1, x2, y2, r = displacements[cell_displacements].T\n",
    "\n",
    "    x_from = min(np.min(x1), np.min(x2))\n",
    "    y_from = min(np.min(y1), np.min(y2))\n",
    "    x_to = max(np.max(x1), np.max(x2))\n",
    "    y_to = max(np.max(y1), np.max(y2))\n",
    "\n",
    "    binning = Binning2D(x1, y1, r)\n",
    "    bins = binning.get_bins(x_from, y_from, x_to, y_to, pixel_bin_size)\n",
    "    counts = np.array([[len(r) for x, y, r in cols] for cols in bins])\n",
    "    fitted_bins = []\n",
    "    fitted_b = []\n",
    "\n",
    "\n",
    "    for row in bins:\n",
    "        fitted = []\n",
    "        for x, y, r in row:\n",
    "            if len(r) > 10: # make sure we have at least 10 displacements\n",
    "                D, b = maximum_likelihood(r, r_max, delta_t, initial_D, initial_b)\n",
    "                fitted.append(D)\n",
    "                fitted_b.append(b)\n",
    "            else:\n",
    "                fitted.append(np.nan)\n",
    "        fitted_bins.append(fitted)\n",
    "        \n",
    "    fitted_bins = np.array(fitted_bins)\n",
    "    \n",
    "        \n",
    "    # Setting the boudarys for Tukey's fences outliers replacement for a median of a dataset\n",
    "    \n",
    "    if outlier_correction:\n",
    "        lower_boundary = np.nanquantile(fitted_bins, 0.25) - k_value * (np.nanquantile(fitted_bins, 0.75) - np.nanquantile(fitted_bins, 0.25))\n",
    "        upper_boundary = np.nanquantile(fitted_bins, 0.75) + k_value * (np.nanquantile(fitted_bins, 0.75) - np.nanquantile(fitted_bins, 0.25))\n",
    "        \n",
    "        fitted_bins = np.where((fitted_bins < lower_boundary) | (fitted_bins > upper_boundary), np.nanmedian(fitted_bins), fitted_bins)                         \n",
    "\n",
    "    fig = plt.figure(figsize=(15,2))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.set_facecolor('#272C36')                                                                                                                         \n",
    "    plt.imshow(fitted_bins, origin=\"lower\", aspect='equal', extent=[x_from, x_to, y_from, y_to], cmap = \"bwr\")\n",
    "    plt.title('Diffusion map')\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(counts, origin=\"lower\", aspect='equal', extent=[x_from, x_to, y_from, y_to])\n",
    "    plt.title('Number of displacements')\n",
    "    plt.colorbar()\n",
    "    path_to_save = path_hdf5[:-5]+\"/Plots/Diffusion maps/\"\n",
    "    plt.savefig(path_to_save+path_hdf5[:-5]+\", Diffusion map and displasements map of cell %a.pdf\" %c)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Size of a bin = %a nm\" % pixel_bin_size_nm)\n",
    "\n",
    "    summary = \"\"\n",
    "        \n",
    "    # check if all fitted_bins are NaN and crates a terminal output\n",
    "    if np.all(np.isnan(fitted_bins)):\n",
    "        summary = \"Not enough displacements per bin. All bins are NaN.\"\n",
    "    else:\n",
    "        summary  = \"Mean diffusion coefficient               : %.2f um^2/s\\n\" % np.nanmean(fitted_bins)\n",
    "        summary += \"Standard deviation diffusion coefficient : %.2f um^2/s\\n\" % np.nanstd(fitted_bins)\n",
    "        summary += \"Minimum diffusion coefficient            : %.2f um^2/s\\n\" % np.nanmin(fitted_bins)\n",
    "        summary += \"Maximum diffusion coefficient            : %.2f um^2/s\\n\" % np.nanmax(fitted_bins)\n",
    "        summary += \"Mean number of displacements per bin     : %d\\n\" % counts.mean()\n",
    "        summary += \"Mean background                          : %.2f\\n\" % np.mean(fitted_b)\n",
    "        summary += \"Standard deviation background            : %.2f\\n\" % np.std(fitted_b)\n",
    "    print(\"Cell number %a\" % (c))\n",
    "    if outlier_correction:\n",
    "        print(\"K-value is %a\" % k_value)\n",
    "    print(summary)\n",
    "    \n",
    "    if Area_selection:          \n",
    "        os.chdir(path_hdf5[:-5])\n",
    "        os.chdir('Plots')\n",
    "        os.chdir('Fitting of areas')\n",
    "        os.chdir('Area selection in cell %a' % c)\n",
    "\n",
    "        Row_to_add_to_df_area = []\n",
    "        Row_to_add_to_df_area.append(int(c))\n",
    "\n",
    "        cell_displacements = displacements[:,1] == c\n",
    "        _, _, x1, y1, x2, y2, r = displacements[cell_displacements].T\n",
    "        width = fitted_bins.shape[1]\n",
    "        height = fitted_bins.shape[0]\n",
    "        start_offset = 0\n",
    "        Cell_slicing()\n",
    "\n",
    "        os.chdir(root_path)\n",
    "\n",
    "        Row_to_add_to_df_area.append(\"As one cell\")\n",
    "        Row_as_serias = pd.Series(Row_to_add_to_df_area, index = df_area.columns)\n",
    "        df_area = df_area.append(Row_as_serias, ignore_index=True)\n",
    "                            \n",
    "# write areas data in a .csv file\n",
    "if Area_selection:\n",
    "    dir_to_save_csv = path_hdf5[:-5]+\"/CSV files/\"\n",
    "    df_area.to_csv(dir_to_save_csv+name_for_df_area, index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
